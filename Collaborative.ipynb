{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType\n",
    "from pyspark.sql.functions import stddev,mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants : Modify as and when required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "business_file=\"Sample_Datasets/montreal_business/part-00000-b5f251d0-79e6-47a8-a405-042eb7b7894e-c000.snappy.parquet\"\n",
    "reviews_file=\"Sample_Datasets/montreal_reviews/part-00000-f0e4463e-0ac9-402e-b995-734cbefc958e-c000.snappy.parquet\"\n",
    "users_file=\"Sample_Datasets/montreal_users/part-00000-a7d49d78-89a7-478f-a577-0efe02dca047-c000.snappy.parquet\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "app_name=\"Collaborative filtering for restaurant recommendation\"\n",
    "\n",
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(app_name) \\\n",
    "        .getOrCreate()\n",
    "    return spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset in Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=init_spark()\n",
    "business_df = spark.read.parquet(business_file)\n",
    "reviews_df=spark.read.parquet(reviews_file)\n",
    "users_df=spark.read.parquet(users_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting required features\n",
    "\n",
    "In our project, we are only concerned with a subset of columns from the dataset, specifically those that are relevant to our goal of recommending restaurants in Montreal. Therefore, we extract the necessary features from the business_df table, including the id, name, stars, category. \n",
    "Similarly, we filter the reviews_df table to include only reviews for the selected restaurants by performing an inner join with business_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df = business_df.select(\"business_id\",\"name\", \"stars\", \n",
    "                                 \"review_count\", \"address\", \"city\", \"state\", \"postal_code\", \"longitude\", \n",
    "                                 \"categories\", \"latitude\").withColumnRenamed(\"stars\", \"stars_restaurant\")\n",
    "reviews_df = reviews_df.join(business_df, on='business_id', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data for ALS: Convert String to index\n",
    "Prior to initiating the modeling process, it is essential to transform all the relevant columns to integer type for compatibility with the ALS model from pyspark. The columns requiring conversion are the business_id and user_id. We accomplish this by leveraging the StringIndexer function, which we imported from pyspark.ml.feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[business_id: string, user_id: string, business_id_index: double, user_id_index: double, stars: double, categories: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in ['business_id', 'user_id']]\n",
    "pipeline = Pipeline(stages=indexer)\n",
    "transformed = pipeline.fit(reviews_df).transform(reviews_df)\n",
    "transformed.select(['business_id', 'user_id','business_id_index', 'user_id_index','stars','categories'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting the dataset into training and testing subsets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting a seed value as 3 to make randomsplit output deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = transformed.randomSplit([0.8, 0.2],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------------+------------+--------------------+--------+-----+-----------+-----------+--------------------+----------+-------------+------------------+\n",
      "|user_id_index|business_id_index|         business_id|             user_id|           review_id|stars|                text|                name|stars_restaurant|review_count|             address|    city|state|postal_code|  longitude|          categories|  latitude|business_mean|         user_mean|\n",
      "+-------------+-----------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------------+------------+--------------------+--------+-----+-----------+-----------+--------------------+----------+-------------+------------------+\n",
      "|        438.0|           1051.0|1O5wKnCuT643BjCu4...|1lRbzzJ5yXGq6K0Cp...|khY0NlhVi8OCmFU9h...|  4.0|I'm a big fan of ...|M4 Burritos Unive...|             3.0|          30|1530 Boul. Maison...|Montréal|   QC|    H3H 2M8|-73.5792117|Mexican, Restaurants|45.4958815|        2.625|3.5454545454545454|\n",
      "|      22436.0|           1051.0|1O5wKnCuT643BjCu4...|4zzt-zBQfHvo-2j1V...|QDcs4SZND6o3go-Y1...|  1.0|Ok, there are gon...|M4 Burritos Unive...|             3.0|          30|1530 Boul. Maison...|Montréal|   QC|    H3H 2M8|-73.5792117|Mexican, Restaurants|45.4958815|        2.625|               1.0|\n",
      "|       4063.0|           1051.0|1O5wKnCuT643BjCu4...|5jGKW1CPY0ltwFJdW...|If0VD8wDmM4QSVO5X...|  3.0|I was window shop...|M4 Burritos Unive...|             3.0|          30|1530 Boul. Maison...|Montréal|   QC|    H3H 2M8|-73.5792117|Mexican, Restaurants|45.4958815|        2.625|               2.6|\n",
      "|       5467.0|           1051.0|1O5wKnCuT643BjCu4...|6QJNlGyGX770yLHm6...|RvdymYEIeP-AzoCUf...|  1.0|Extra charges for...|M4 Burritos Unive...|             3.0|          30|1530 Boul. Maison...|Montréal|   QC|    H3H 2M8|-73.5792117|Mexican, Restaurants|45.4958815|        2.625|              3.25|\n",
      "|        263.0|           1051.0|1O5wKnCuT643BjCu4...|EaBKe-8LB-NHuH7Us...|M-qeKlumvuVKmyzKI...|  2.0|Ok, this is just ...|M4 Burritos Unive...|             3.0|          30|1530 Boul. Maison...|Montréal|   QC|    H3H 2M8|-73.5792117|Mexican, Restaurants|45.4958815|        2.625| 3.973684210526316|\n",
      "|       1030.0|           1051.0|1O5wKnCuT643BjCu4...|F5P_YyFeucqKu00lm...|uCwlvOCMPVmX6m8ra...|  4.0|The burritos here...|M4 Burritos Unive...|             3.0|          30|1530 Boul. Maison...|Montréal|   QC|    H3H 2M8|-73.5792117|Mexican, Restaurants|45.4958815|        2.625| 4.111111111111111|\n",
      "|        489.0|           1051.0|1O5wKnCuT643BjCu4...|GOVm1Ba9xx5JNKfMq...|FI0fe5Ro9RPU0pjyp...|  4.0|This place is pre...|M4 Burritos Unive...|             3.0|          30|1530 Boul. Maison...|Montréal|   QC|    H3H 2M8|-73.5792117|Mexican, Restaurants|45.4958815|        2.625| 4.045454545454546|\n",
      "|       3360.0|           1051.0|1O5wKnCuT643BjCu4...|HOkkqfN8KpEdliCwk...|7FSOw9WVCBmcKQBRo...|  2.0|What a ridiculous...|M4 Burritos Unive...|             3.0|          30|1530 Boul. Maison...|Montréal|   QC|    H3H 2M8|-73.5792117|Mexican, Restaurants|45.4958815|        2.625|               3.4|\n",
      "|        393.0|           1051.0|1O5wKnCuT643BjCu4...|MeJJ50cVS-ycShR5w...|4jfJATeKskTaxeVDx...|  2.0|My friends and I ...|M4 Burritos Unive...|             3.0|          30|1530 Boul. Maison...|Montréal|   QC|    H3H 2M8|-73.5792117|Mexican, Restaurants|45.4958815|        2.625|3.5517241379310347|\n",
      "|      14430.0|           1051.0|1O5wKnCuT643BjCu4...|NPGoSMd_rkvJCU2qb...|qZEKjR9QC4Sr4EIIC...|  1.0|Been going here f...|M4 Burritos Unive...|             3.0|          30|1530 Boul. Maison...|Montréal|   QC|    H3H 2M8|-73.5792117|Mexican, Restaurants|45.4958815|        2.625|               1.0|\n",
      "+-------------+-----------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+----------------+------------+--------------------+--------+-----+-----------+-----------+--------------------+----------+-------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "None\n",
      "3.8900512267457534\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "# Compute user mean and item mean on the training set\n",
    "user_mean_ratings = training.groupBy('user_id_index').agg(avg('stars').alias('user_mean'))\n",
    "business_mean = training.groupBy('business_id_index').agg(avg('stars').alias('business_mean'))\n",
    "\n",
    "# Compute the global mean\n",
    "global_mean = training.agg(avg('stars')).collect()[0][0]\n",
    "\n",
    "training_item = training.join(business_mean, \"business_id_index\")\n",
    "training_item = training_item.withColumnRenamed(\"avg(rating)\", \"business_mean\")\n",
    "training_item_user = training_item.join(user_mean_ratings, \"user_id_index\")\n",
    "training_item_user = training_item_user.withColumnRenamed(\"avg(rating)\", \"user_mean\")\n",
    "training_item_user_interaction = training_item_user.withColumn(\"user_item_interaction\", training_item_user.stars - (training_item_user.user_mean + training_item_user.business_mean - global_mean))\n",
    "training_item_user_interaction = training_item_user_interaction.drop(\"timestamp\").sort(\"user_id_index\", \"business_id_index\")\n",
    "\n",
    "print(training_item_user.show(10))\n",
    "print(global_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ALS model\n",
    "The Apache Spark library provides various parameters for the ALS (Alternating Least Squares) algorithm, including:\n",
    "\n",
    "- rank: the number of latent factors used in the model (default value: 10).\n",
    "- maxIter: the maximum number of iterations to run (default value: 10).\n",
    "- regParam: the regularization parameter used in ALS (default value: 1.0).\n",
    "- implicitPrefs: a boolean value that indicates whether to use the explicit feedback ALS variant or the one adapted for implicit feedback data (default value: false, which means using explicit feedback).\n",
    "- alpha: a parameter that applies to the implicit feedback variant of ALS, determining the baseline confidence in preference observations (default value: 1.0).\n",
    "- nonnegative: a boolean value that specifies whether to use nonnegative constraints for least squares (default value: false)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "user_item_interaction does not exist. Available: business_id, user_id, review_id, stars, text, name, stars_restaurant, review_count, address, city, state, postal_code, longitude, categories, latitude, business_id_index, user_id_index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m als\u001b[39m=\u001b[39mALS(maxIter\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m,\n\u001b[0;32m      2\u001b[0m         regParam\u001b[39m=\u001b[39m\u001b[39m0.09\u001b[39m,\n\u001b[0;32m      3\u001b[0m         rank\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m         coldStartStrategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdrop\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m         nonnegative\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 10\u001b[0m model\u001b[39m=\u001b[39mals\u001b[39m.\u001b[39;49mfit(training)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[0;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pyspark\\ml\\wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[1;32m--> 383\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[0;32m    384\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    385\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pyspark\\ml\\wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 380\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pyspark\\sql\\utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: user_item_interaction does not exist. Available: business_id, user_id, review_id, stars, text, name, stars_restaurant, review_count, address, city, state, postal_code, longitude, categories, latitude, business_id_index, user_id_index"
     ]
    }
   ],
   "source": [
    "als=ALS(maxIter=5,\n",
    "        regParam=0.09,\n",
    "        rank=20,\n",
    "        userCol=\"user_id_index\",\n",
    "        itemCol=\"business_id_index\",\n",
    "        ratingCol=\"stars\",\n",
    "        coldStartStrategy=\"drop\",\n",
    "        nonnegative=True)\n",
    "\n",
    "model=als.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business_id', 'user_id', 'review_id', 'stars', 'text', 'name', 'stars_restaurant', 'review_count', 'address', 'city', 'state', 'postal_code', 'longitude', 'categories', 'latitude', 'business_id_index', 'user_id_index', 'prediction']\n",
      "RMSE=1.4427854515173137\n"
     ]
    }
   ],
   "source": [
    "evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"stars\",predictionCol=\"prediction\")\n",
    "predictions=model.transform(test)\n",
    "print(predictions.columns)\n",
    "rmse=evaluator.evaluate(predictions)\n",
    "print(\"RMSE=\"+str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values=[0.08,0.09,0.1,0.15,0.2,0.25,0.3,0.4,0.45,0.5,0.5,0.6,0.7,0.8,0.9]\n",
    "for rm in values:\n",
    "    als = ALS(maxIter=20,regParam=rm,rank=25,userCol=\"user_id_index\",itemCol=\"business_id_index\",ratingCol=\"stars\",coldStartStrategy=\"drop\",nonnegative=True)\n",
    "    model = als.fit(training)\n",
    "    evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"stars\",predictionCol=\"prediction\")\n",
    "    predictions=model.transform(test)\n",
    "    rmse=evaluator.evaluate(predictions)\n",
    "    print(\"RMSE=\"+str(rmse))\n",
    "    print(model.userFactors.count())\n",
    "    print(model.itemFactors.count())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are uisng 0.4 as the regularizatin parameter as it yields lowest RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE=1.2328262876620237\n",
      "Given ratings vs Predicted ratings\n",
      "+--------------------+--------------------+-----+----------+\n",
      "|             user_id|         business_id|stars|prediction|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "|KcZOGv_2fnDZyRTnf...|-1xuC540Nycht_iWF...|  4.0|  4.227715|\n",
      "|f36zzHyedEUiXqLGg...|-iQdfdTpXa3Y-WC5O...|  5.0| 3.1735282|\n",
      "|WxSV-4GoqPCczNn4R...|0S46SVEicJjWzqMOI...|  5.0| 1.7627081|\n",
      "|6KHdW3MjBZJZ4BkzJ...|0W4lkclzZThpx3V65...|  4.0| 3.5963745|\n",
      "|75Xtdm65_xnFXbL3v...|0W4lkclzZThpx3V65...|  5.0| 4.0726385|\n",
      "|C7o1LcGjQis0ICvsQ...|0W4lkclzZThpx3V65...|  4.0| 3.5397718|\n",
      "|D-jIQQUo6bz-sdVez...|0W4lkclzZThpx3V65...|  5.0| 3.7106848|\n",
      "|2av488ePvb-Z4qgeN...|25m6rM6hFw2CGADUj...|  4.0|  3.725397|\n",
      "|_sEaXXa7gopr8xEK6...|2LOTz08OPRyA0ibJI...|  3.0|  4.291952|\n",
      "|xpvXXjfUDVulJODsu...|2meb5cwKh125tF5Iw...|  4.0|  4.495398|\n",
      "|alUuOskFSl1bODjnc...|3LTP2zkDyde28b3ny...|  4.0| 1.8666558|\n",
      "|mJ6yNAm5x5-lyW7mk...|3jKUbhGSjFTv5jZ0w...|  4.0| 3.2019541|\n",
      "|2av488ePvb-Z4qgeN...|46Ld9Qc9nAx_A0jwc...|  4.0|  3.749227|\n",
      "|NbhqefRgs8bGq25NF...|46Ld9Qc9nAx_A0jwc...|  5.0| 4.3886914|\n",
      "|vZUpa5C8JE014KDri...|4vzxDkAcnOQEQcs4r...|  2.0| 2.8436258|\n",
      "|lwLL8zI-1FL_WDWTJ...|5T6kFKFycym_GkhgO...|  5.0|  4.323325|\n",
      "|qeT7hJoZRtGuUuaL2...|66LvqsvQ3niOFqJIP...|  4.0| 2.8165796|\n",
      "|mJ6yNAm5x5-lyW7mk...|6BL92dndWIhhyA7KZ...|  3.0| 3.3252094|\n",
      "|KcZOGv_2fnDZyRTnf...|6fd9EIo9TAFvJLkfC...|  3.0|  4.337807|\n",
      "|EPsWdQzalccFouFC4...|7hxwnxLCnnvWKWPCn...|  3.0| 3.1910195|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "als = ALS(maxIter=10,regParam=0.4,rank=25,userCol=\"user_id_index\",itemCol=\"business_id_index\",ratingCol=\"stars\",coldStartStrategy=\"drop\",nonnegative=True)\n",
    "model = als.fit(training)\n",
    "evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"stars\",predictionCol=\"prediction\")\n",
    "predictions=model.transform(test)\n",
    "rmse=evaluator.evaluate(predictions)\n",
    "print(\"RMSE=\"+str(rmse))\n",
    "print(\"Given ratings vs Predicted ratings\")\n",
    "predictions.select(['user_id', 'business_id', 'stars', 'prediction']).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = predictions.select(stddev('prediction')).collect()[0][0]\n",
    "print('Standard deviation of predictions:', std_dev)\n",
    "\n",
    "std_dev_stars = predictions.select(stddev('stars')).collect()[0][0]\n",
    "print('Standard deviation of stars:', std_dev_stars)\n",
    "\n",
    "mean_prediction = predictions.select(mean('prediction')).collect()[0][0]\n",
    "print('Mean of predictions:', mean_prediction)\n",
    "\n",
    "mean_stars = predictions.select(mean('stars')).collect()[0][0]\n",
    "print('Mean of stars:', mean_stars)\n",
    "\n",
    "print('Standard Deviation ratio ', std_dev/std_dev_stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Best Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations():\n",
    "    \"\"\"\n",
    "    Returns top recommendations for a user.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    :py:class:`pyspark.sql.DataFrame`\n",
    "    a DataFrame of (itemCol, recommendations), where recommendations are\n",
    "    stored as an array of ('name','business_id', 'stars', 'categories') Rows.\n",
    "    \"\"\"\n",
    "    test = model.recommendForAllUsers(10).filter(col('user_id_index')==30).select(\"recommendations\").take(10)\n",
    "    topRestaurants = []\n",
    "    for item in test[0][0]:        \n",
    "        topRestaurants.append(item.business_id_index)\n",
    "    \n",
    "    schema = StructType([StructField(\"business_id_index\",IntegerType(),True)])\n",
    "    restaurants = spark.createDataFrame(topRestaurants,IntegerType()).toDF(\"business_id_index\")\n",
    "    return restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_transformed_list():\n",
    "    transformed\\\n",
    "    .select(['name', 'user_id', 'stars', 'categories'])\\\n",
    "    .filter(col('user_id_index')==30)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_top10_recommendations(restaurants):\n",
    "    \"\"\"\n",
    "    Displays the top 10 restaurant recommendations.\n",
    "    \"\"\"\n",
    "    restaurants\\\n",
    "    .join(transformed, on = 'business_id_index', how = 'inner')\\\n",
    "    .select([ 'name','business_id', 'stars', 'categories'])\\\n",
    "    .drop_duplicates(subset=['name'])\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Top Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "top10_recommendations = get_recommendations()\n",
    "display_top10_recommendations(top10_recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
