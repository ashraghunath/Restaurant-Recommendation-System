{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants : Modify as and when required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "business_file=\"Sample_Datasets/montreal_business/part-00000-b5f251d0-79e6-47a8-a405-042eb7b7894e-c000.snappy.parquet\"\n",
    "reviews_file=\"Sample_Datasets/montreal_reviews/part-00000-f0e4463e-0ac9-402e-b995-734cbefc958e-c000.snappy.parquet\"\n",
    "users_file=\"Sample_Datasets/montreal_users/part-00000-a7d49d78-89a7-478f-a577-0efe02dca047-c000.snappy.parquet\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "app_name=\"Collaborative filtering for restaurant recommendation\"\n",
    "\n",
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(app_name) \\\n",
    "        .getOrCreate()\n",
    "    return spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset in Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=init_spark()\n",
    "business_df = spark.read.parquet(business_file)\n",
    "reviews_df=spark.read.parquet(reviews_file)\n",
    "users_df=spark.read.parquet(users_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Standard Deviation of the ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2240193031101316\n",
      "3.890370817263149\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import stddev,mean\n",
    "std_dev = reviews_df.select(stddev('stars')).collect()[0][0]\n",
    "avg = reviews_df.select(mean('stars')).collect()[0][0]\n",
    "\n",
    "print(std_dev)\n",
    "print(avg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting required features\n",
    "\n",
    "In our project, we are only concerned with a subset of columns from the dataset, specifically those that are relevant to our goal of recommending restaurants in Montreal. Therefore, we extract the necessary features from the business_df table, including the id, name, stars, category. \n",
    "Similarly, we filter the reviews_df table to include only reviews for the selected restaurants by performing an inner join with business_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df = business_df.select(\"business_id\",\"name\", \"stars\", \n",
    "                                 \"review_count\", \"address\", \"city\", \"state\", \"postal_code\", \"longitude\", \n",
    "                                 \"categories\", \"latitude\").withColumnRenamed(\"stars\", \"stars_restaurant\")\n",
    "reviews_df = reviews_df.join(business_df, on='business_id', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data for ALS: Convert String to index\n",
    "Prior to initiating the modeling process, it is essential to transform all the relevant columns to integer type for compatibility with the ALS model from pyspark. The columns requiring conversion are the business_id and user_id. We accomplish this by leveraging the StringIndexer function, which we imported from pyspark.ml.feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[business_id: string, user_id: string, business_id_index: double, user_id_index: double, stars: double, categories: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in ['business_id', 'user_id']]\n",
    "pipeline = Pipeline(stages=indexer)\n",
    "transformed = pipeline.fit(reviews_df).transform(reviews_df)\n",
    "transformed.select(['business_id', 'user_id','business_id_index', 'user_id_index','stars','categories'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting the dataset into training and testing subsets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting a seed value as 3 to make randomsplit output deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = transformed.randomSplit([0.8, 0.2],3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ALS model\n",
    "The Apache Spark library provides various parameters for the ALS (Alternating Least Squares) algorithm, including:\n",
    "\n",
    "- rank: the number of latent factors used in the model (default value: 10).\n",
    "- maxIter: the maximum number of iterations to run (default value: 10).\n",
    "- regParam: the regularization parameter used in ALS (default value: 1.0).\n",
    "- implicitPrefs: a boolean value that indicates whether to use the explicit feedback ALS variant or the one adapted for implicit feedback data (default value: false, which means using explicit feedback).\n",
    "- alpha: a parameter that applies to the implicit feedback variant of ALS, determining the baseline confidence in preference observations (default value: 1.0).\n",
    "- nonnegative: a boolean value that specifies whether to use nonnegative constraints for least squares (default value: false)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "als=ALS(maxIter=5,\n",
    "        regParam=0.09,\n",
    "        rank=20,\n",
    "        userCol=\"user_id_index\",\n",
    "        itemCol=\"business_id_index\",\n",
    "        ratingCol=\"stars\",\n",
    "        coldStartStrategy=\"drop\",\n",
    "        nonnegative=True)\n",
    "\n",
    "model=als.fit(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"stars\",predictionCol=\"prediction\")\n",
    "predictions=model.transform(test)\n",
    "rmse=evaluator.evaluate(predictions)\n",
    "print(\"RMSE=\"+str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values=[0.08,0.09,0.1,0.15,0.2,0.25,0.3,0.4,0.45,0.5,0.5,0.6,0.7,0.8,0.9]\n",
    "for rm in values:\n",
    "    als = ALS(maxIter=20,regParam=rm,rank=25,userCol=\"user_id_index\",itemCol=\"business_id_index\",ratingCol=\"stars\",coldStartStrategy=\"drop\",nonnegative=True)\n",
    "    model = als.fit(training)\n",
    "    evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"stars\",predictionCol=\"prediction\")\n",
    "    predictions=model.transform(test)\n",
    "    rmse=evaluator.evaluate(predictions)\n",
    "    print(\"RMSE=\"+str(rmse))\n",
    "    print(model.userFactors.count())\n",
    "    print(model.itemFactors.count())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are uisng 0.4 as the regularizatin parameter as it yields lowest RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE=1.236113620077952\n",
      "Given ratings vs Predicted ratings\n",
      "+--------------------+--------------------+-----+----------+\n",
      "|             user_id|         business_id|stars|prediction|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "|ysCBsXWPB-LAiewVS...|-WLV_MlPGVy4M98Xv...|  4.0| 2.2992718|\n",
      "|3AxAep0OMmy_Lgids...|1cRvhC7shC6UneMqR...|  5.0| 3.4202719|\n",
      "|8ptxwJ0c2pY0aQ6Pq...|1cRvhC7shC6UneMqR...|  4.0|  3.191821|\n",
      "|xEzPJcx5Ny6NLm0p9...|1cRvhC7shC6UneMqR...|  3.0| 3.6124904|\n",
      "|Vav-GApWFEVD5wGyK...|3vVHoQg2rCSWOq3yV...|  3.0| 2.7110302|\n",
      "|hVb9u3kjXVxGZYJ74...|3vVHoQg2rCSWOq3yV...|  4.0| 2.7092052|\n",
      "|zIOxNinC6ofIwwGjO...|3vVHoQg2rCSWOq3yV...|  2.0|   2.21408|\n",
      "|7kGxOoefe5H-HMxKI...|5KsvP10z9InBcI6hh...|  2.0|  1.802579|\n",
      "|8jsRaddgwE26drgje...|5KsvP10z9InBcI6hh...|  1.0| 0.8365478|\n",
      "|9mR8YvTA3tJJL3K9B...|5KsvP10z9InBcI6hh...|  1.0| 2.5298412|\n",
      "|GpV7fg9-iadPX5HI5...|5KsvP10z9InBcI6hh...|  3.0|  2.029737|\n",
      "|OP7nO0dIaYW0O6DYX...|5KsvP10z9InBcI6hh...|  2.0|  2.369381|\n",
      "|OP7nO0dIaYW0O6DYX...|5KsvP10z9InBcI6hh...|  1.0|  2.369381|\n",
      "|VoKCmDRJ_qtNFfL7V...|5KsvP10z9InBcI6hh...|  3.0| 1.7981328|\n",
      "|YEGEwxbynVQCTVWr_...|5KsvP10z9InBcI6hh...|  1.0| 2.4951808|\n",
      "|gVIg5YIIx44tz4fTz...|5KsvP10z9InBcI6hh...|  2.0| 2.2015057|\n",
      "|oYSLehM5rPWw2_sZT...|5KsvP10z9InBcI6hh...|  2.0| 2.0912325|\n",
      "|488KrW58ryVduy0Me...|5NRLnaFBxO4V7bN5E...|  3.0| 3.1700811|\n",
      "|9dYJu74tHGBXrqi1c...|5NRLnaFBxO4V7bN5E...|  3.0| 2.7469456|\n",
      "|QBe3oadPKot2mtuh8...|5NRLnaFBxO4V7bN5E...|  4.0| 2.7130291|\n",
      "+--------------------+--------------------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "als = ALS(maxIter=20,regParam=0.4,rank=25,userCol=\"user_id_index\",itemCol=\"business_id_index\",ratingCol=\"stars\",coldStartStrategy=\"drop\",nonnegative=True)\n",
    "model = als.fit(training)\n",
    "evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"stars\",predictionCol=\"prediction\")\n",
    "predictions=model.transform(test)\n",
    "rmse=evaluator.evaluate(predictions)\n",
    "print(\"RMSE=\"+str(rmse))\n",
    "print(\"Given ratings vs Predicted ratings\")\n",
    "predictions.select(['user_id', 'business_id', 'stars', 'prediction']).show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation of predicted ratings:  1.236113620077952\n",
      "Mean rating:  3.8865761405816848\n"
     ]
    }
   ],
   "source": [
    "std_dev_model = predictions.selectExpr(\"sqrt(avg((stars-prediction)*(stars-prediction)))\").collect()[0][0]\n",
    "avg_rating_model = predictions.selectExpr(\"avg(stars)\").collect()[0][0]\n",
    "\n",
    "print(\"Standard deviation of predicted ratings: \", std_dev_model)\n",
    "print(\"Mean rating: \", avg_rating_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference Between Actual vs predicted\n",
      "Standard Deviation: \n",
      " from dataset: 1.2240193031101316 from model:1.236113620077952 Difference: -0.012094316967820262\n",
      "Average Rating: \n",
      " from dataset: 3.890370817263149 from model:3.8865761405816848 Difference: 0.0037946766814642707\n"
     ]
    }
   ],
   "source": [
    "print(\"Difference Between Actual vs predicted\")\n",
    "print(f\"Standard Deviation: \\n from dataset: {std_dev} from model:{std_dev_model} Difference: {std_dev-std_dev_model}\")\n",
    "print(f\"Average Rating: \\n from dataset: {avg} from model:{avg_rating_model} Difference: {avg-avg_rating_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Best Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations():\n",
    "    \"\"\"\n",
    "    Returns top recommendations for a user.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    :py:class:`pyspark.sql.DataFrame`\n",
    "    a DataFrame of (itemCol, recommendations), where recommendations are\n",
    "    stored as an array of ('name','business_id', 'stars', 'categories') Rows.\n",
    "    \"\"\"\n",
    "    test = model.recommendForAllUsers(10).filter(col('user_id_index')==30).select(\"recommendations\").take(10)\n",
    "    topRestaurants = []\n",
    "    for item in test[0][0]:        \n",
    "        topRestaurants.append(item.business_id_index)\n",
    "    \n",
    "    schema = StructType([StructField(\"business_id_index\",IntegerType(),True)])\n",
    "    restaurants = spark.createDataFrame(topRestaurants,IntegerType()).toDF(\"business_id_index\")\n",
    "    return restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_transformed_list():\n",
    "    transformed\\\n",
    "    .select(['name', 'user_id', 'stars', 'categories'])\\\n",
    "    .filter(col('user_id_index')==30)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_top10_recommendations(restaurants):\n",
    "    \"\"\"\n",
    "    Displays the top 10 restaurant recommendations.\n",
    "    \"\"\"\n",
    "    restaurants\\\n",
    "    .join(transformed, on = 'business_id_index', how = 'inner')\\\n",
    "    .select([ 'name','business_id', 'stars', 'categories'])\\\n",
    "    .drop_duplicates(subset=['name'])\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Top Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "top10_recommendations = get_recommendations()\n",
    "display_top10_recommendations(top10_recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
