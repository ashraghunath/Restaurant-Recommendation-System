{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType\n",
    "from pyspark.sql.functions import stddev,mean,avg,lit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampled data-file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "business_file=\"Sample_Datasets/montreal_business/part-00000-b5f251d0-79e6-47a8-a405-042eb7b7894e-c000.snappy.parquet\"\n",
    "reviews_file=\"Sample_Datasets/montreal_reviews/part-00000-f0e4463e-0ac9-402e-b995-734cbefc958e-c000.snappy.parquet\"\n",
    "users_file=\"Sample_Datasets/montreal_users/part-00000-a7d49d78-89a7-478f-a577-0efe02dca047-c000.snappy.parquet\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "app_name=\"Collaborative filtering for restaurant recommendation\"\n",
    "\n",
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(app_name) \\\n",
    "        .getOrCreate()\n",
    "    return spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset in Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=init_spark()\n",
    "business_df = spark.read.parquet(business_file)\n",
    "reviews_df=spark.read.parquet(reviews_file)\n",
    "users_df=spark.read.parquet(users_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting required features\n",
    "\n",
    "In our project, we are only concerned with a subset of columns from the dataset, specifically those that are relevant to our goal of recommending restaurants in Montreal. Therefore, we extract the necessary features from the business_df table, including the id, name, stars, category. \n",
    "Similarly, we filter the reviews_df table to include only reviews for the selected restaurants by performing an inner join with business_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df = business_df.select(\"business_id\",\"name\", \"stars\", \n",
    "                                 \"review_count\", \"address\", \"city\", \"state\", \"postal_code\", \"longitude\", \n",
    "                                 \"categories\", \"latitude\").withColumnRenamed(\"stars\", \"stars_restaurant\")\n",
    "reviews_df = reviews_df.join(business_df, on='business_id', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data for ALS: Convert String to index\n",
    "Prior to initiating the modeling process, it is essential to transform all the relevant columns to integer type for compatibility with the ALS model from pyspark. The columns requiring conversion are the business_id and user_id. We accomplish this by leveraging the StringIndexer function, which we imported from pyspark.ml.feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[business_id: string, user_id: string, business_id_index: double, user_id_index: double, stars: double, categories: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexer = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in ['business_id', 'user_id']]\n",
    "pipeline = Pipeline(stages=indexer)\n",
    "transformed = pipeline.fit(reviews_df).transform(reviews_df)\n",
    "transformed.select(['business_id', 'user_id','business_id_index', 'user_id_index','stars','categories'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting the dataset into training and testing subsets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting a seed value as 3 to make randomsplit output deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = transformed.randomSplit([0.8, 0.2],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Average: 3.8900512267457534\n",
      "Global Average: 3.8900512267457534\n"
     ]
    }
   ],
   "source": [
    "# Compute the global mean\n",
    "average = training.select('user_id','stars').withColumn(\"user_id\",lit(1)).groupBy('user_id').mean()\n",
    "global_average = average.select('avg(stars)').collect()[0][0]\n",
    "\n",
    "print(\"Global Average:\",str(global_average))\n",
    "global_mean = training.agg(avg('stars')).collect()[0][0]\n",
    "print(\"Global Average:\",str(global_mean))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction using global average for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.2241497595291404\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the root mean squared error if we use the global average as a prediction for all rating, Our model should perform better than this\n",
    "\n",
    "\n",
    "test_avg = test.withColumn('prediction',lit(global_mean))\n",
    "    \n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"stars\",predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(test_avg)\n",
    "\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic ALS model\n",
    "The Apache Spark library provides various parameters for the ALS (Alternating Least Squares) algorithm, including:\n",
    "\n",
    "- rank: the number of latent factors used in the model (default value: 10).\n",
    "- maxIter: the maximum number of iterations to run (default value: 10).\n",
    "- regParam: the regularization parameter used in ALS (default value: 1.0).\n",
    "- implicitPrefs: a boolean value that indicates whether to use the explicit feedback ALS variant or the one adapted for implicit feedback data (default value: false, which means using explicit feedback).\n",
    "- alpha: a parameter that applies to the implicit feedback variant of ALS, determining the baseline confidence in preference observations (default value: 1.0).\n",
    "- nonnegative: a boolean value that specifies whether to use nonnegative constraints for least squares (default value: false)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ALS_5da17d99a415"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#ALS model with default values\n",
    "als=ALS(userCol=\"user_id_index\",itemCol=\"business_id_index\",ratingCol=\"stars\",coldStartStrategy=\"drop\",nonnegative=True)\n",
    "als.setSeed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the default model\n",
    "model=als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the stars for test set \n",
    "predictions=model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE=1.3551268880423306\n"
     ]
    }
   ],
   "source": [
    "#evaludate default model\n",
    "evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"stars\",predictionCol=\"prediction\")\n",
    "rmse=evaluator.evaluate(predictions)\n",
    "print(\"RMSE=\"+str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE with latent factor 5 is=1.3540113481147706\n",
      "RMSE with latent factor 30 is=1.3264383748499142\n",
      "RMSE with latent factor 50 is=1.3190894049038684\n",
      "RMSE with latent factor 100 is=1.3116132345110683\n"
     ]
    }
   ],
   "source": [
    "ranks=[5,30,50,100]\n",
    "for rank in ranks:\n",
    "    als = ALS(rank=rank,userCol=\"user_id_index\",itemCol=\"business_id_index\",ratingCol=\"stars\",coldStartStrategy=\"drop\",nonnegative=True)\n",
    "    als.setSeed(2)\n",
    "    model = als.fit(training)\n",
    "    evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"stars\",predictionCol=\"prediction\")\n",
    "    predictions=model.transform(test)\n",
    "    rmse=evaluator.evaluate(predictions)\n",
    "    print(\"RMSE with latent factor \"+str(rank) +\" is=\"+str(rmse))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epcohs=[10,15,20]\n",
    "for epcoh in epcohs:\n",
    "    als = ALS(rank=100,maxIter=epcoh,userCol=\"user_id_index\",itemCol=\"business_id_index\",ratingCol=\"stars\",coldStartStrategy=\"drop\",nonnegative=True)\n",
    "    als.setSeed(2)\n",
    "    model = als.fit(training)\n",
    "    evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"stars\",predictionCol=\"prediction\")\n",
    "    predictions=model.transform(test)\n",
    "    rmse=evaluator.evaluate(predictions)\n",
    "    print(\"RMSE with latent factor 100 and maxIter \"+str(epcoh) +\" is=\"+str(rmse))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALS Model With Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute user mean and item mean on the training set\n",
    "user_mean = training.groupBy('user_id_index').agg(avg('stars').alias('user_mean'))\n",
    "item_mean = training.groupBy('business_id_index').agg(avg('stars').alias('item_mean'))\n",
    "#remove bias from training set\n",
    "interactions = training.join(user_mean, 'user_id_index').join(item_mean, 'business_id_index')\n",
    "interactions = interactions.withColumn('user_item_interaction', col('stars') - col('user_mean') - col('item_mean') + global_mean)\n",
    "interactions.select('stars','user_mean','item_mean','user_item_interaction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='stars', predictionCol='user_item_interaction_recomputed')\n",
    "\n",
    "ranks=[5,10,30,50,100]\n",
    "for rank in ranks:\n",
    "    als = ALS(maxIter=20,rank=rank,userCol=\"user_id_index\",itemCol=\"business_id_index\",ratingCol=\"user_item_interaction\",coldStartStrategy=\"drop\",nonnegative=True)\n",
    "    als.setSeed(2)\n",
    "    model = als.fit(interactions)\n",
    "    predictions=model.transform(test).withColumnRenamed('prediction','predicted_rating')\n",
    "    predictions=  predictions.join(user_mean, 'user_id_index').join(item_mean, 'business_id_index')\n",
    "    predictions = predictions.withColumn('user_item_interaction_recomputed', col('predicted_rating') + col('user_mean') + col('item_mean') - global_mean)\n",
    "    rmse=evaluator.evaluate(predictions)\n",
    "    print(\"RMSE with latent factor \"+str(rank) +\" is=\"+str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(maxIter=20,rank=100,userCol=\"user_id_index\",itemCol=\"business_id_index\",ratingCol=\"user_item_interaction\",coldStartStrategy=\"drop\",nonnegative=True)\n",
    "als.setSeed(2)\n",
    "model = als.fit(interactions)\n",
    "predictions=model.transform(test).withColumnRenamed('prediction','predicted_rating')\n",
    "predictions=  predictions.join(user_mean, 'user_id_index').join(item_mean, 'business_id_index')\n",
    "predictions = predictions.withColumn('user_item_interaction_recomputed', col('predicted_rating') + col('user_mean') + col('item_mean') - global_mean)\n",
    "  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='stars', predictionCol='user_item_interaction_recomputed')\n",
    "rmse=evaluator.evaluate(predictions)\n",
    "print(\"RMSE is=\"+str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = predictions.select(stddev('user_item_interaction_recomputed')).collect()[0][0]\n",
    "print('Standard deviation of predictions:', std_dev)\n",
    "\n",
    "std_dev_stars = predictions.select(stddev('stars')).collect()[0][0]\n",
    "print('Standard deviation of stars:', std_dev_stars)\n",
    "\n",
    "mean_prediction = predictions.select(mean('user_item_interaction_recomputed')).collect()[0][0]\n",
    "print('Mean of predictions:', mean_prediction)\n",
    "\n",
    "mean_stars = predictions.select(mean('stars')).collect()[0][0]\n",
    "print('Mean of stars:', mean_stars)\n",
    "\n",
    "print('Standard Deviation ratio ', std_dev/std_dev_stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Best Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations():\n",
    "    \"\"\"\n",
    "    Returns top recommendations for a user.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    :py:class:`pyspark.sql.DataFrame`\n",
    "    a DataFrame of (itemCol, recommendations), where recommendations are\n",
    "    stored as an array of ('name','business_id', 'stars', 'categories') Rows.\n",
    "    \"\"\"\n",
    "    test = model.recommendForAllUsers(10).filter(col('user_id_index')==30).select(\"recommendations\").take(10)\n",
    "    topRestaurants = []\n",
    "    for item in test[0][0]:        \n",
    "        topRestaurants.append(item.business_id_index)\n",
    "    \n",
    "    schema = StructType([StructField(\"business_id_index\",IntegerType(),True)])\n",
    "    restaurants = spark.createDataFrame(topRestaurants,IntegerType()).toDF(\"business_id_index\")\n",
    "    return restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_transformed_list():\n",
    "    transformed\\\n",
    "    .select(['name', 'user_id', 'stars', 'categories'])\\\n",
    "    .filter(col('user_id_index')==30)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_top10_recommendations(restaurants):\n",
    "    \"\"\"\n",
    "    Displays the top 10 restaurant recommendations.\n",
    "    \"\"\"\n",
    "    restaurants\\\n",
    "    .join(transformed, on = 'business_id_index', how = 'inner')\\\n",
    "    .select([ 'name','business_id', 'stars', 'categories'])\\\n",
    "    .drop_duplicates(subset=['name'])\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Top Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "top10_recommendations = get_recommendations()\n",
    "display_top10_recommendations(top10_recommendations)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
