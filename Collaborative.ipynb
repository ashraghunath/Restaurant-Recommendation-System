{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType\n",
    "from pyspark.sql.functions import stddev,mean,avg,lit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample data file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths\n",
    "business_file=\"Sample_Datasets/montreal_business/part-00000-b5f251d0-79e6-47a8-a405-042eb7b7894e-c000.snappy.parquet\"\n",
    "reviews_file=\"Sample_Datasets/montreal_reviews/part-00000-f0e4463e-0ac9-402e-b995-734cbefc958e-c000.snappy.parquet\"\n",
    "users_file=\"Sample_Datasets/montreal_users/part-00000-a7d49d78-89a7-478f-a577-0efe02dca047-c000.snappy.parquet\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "app_name=\"Collaborative filtering for restaurant recommendation\"\n",
    "\n",
    "def init_spark():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(app_name) \\\n",
    "        .getOrCreate()\n",
    "    return spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset in Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=init_spark()\n",
    "business_df = spark.read.parquet(business_file)\n",
    "reviews_df=spark.read.parquet(reviews_file)\n",
    "users_df=spark.read.parquet(users_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting required features\n",
    "\n",
    "In our project, we are only concerned with a subset of columns from the dataset, specifically those that are relevant to our goal of recommending restaurants in Montreal. Therefore, we extract the necessary features from the business_df table, including the id, name, stars, category. \n",
    "Similarly, we filter the reviews_df table to include only reviews for the selected restaurants by performing an inner join with business_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_df = business_df.select(\"business_id\",\"name\", \"stars\", \n",
    "                                 \"review_count\", \"address\", \"city\", \"state\", \"postal_code\", \"longitude\", \n",
    "                                 \"categories\", \"latitude\").withColumnRenamed(\"stars\", \"stars_restaurant\")\n",
    "reviews_df = reviews_df.join(business_df, on='business_id', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data for ALS: Convert String to index\n",
    "Prior to initiating the modeling process, it is essential to transform all the relevant columns to integer type for compatibility with the ALS model from pyspark. The columns requiring conversion are the business_id and user_id. We accomplish this by leveraging the StringIndexer function, which we imported from pyspark.ml.feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o736.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 111.0 failed 1 times, most recent failure: Lost task 0.0 in stage 111.0 (TID 454) (192.168.0.134 executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:424)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:345)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:373)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:345)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3120)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3120)\r\n\tat org.apache.spark.ml.feature.StringIndexer.countByValue(StringIndexer.scala:204)\r\n\tat org.apache.spark.ml.feature.StringIndexer.sortByFreq(StringIndexer.scala:212)\r\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:242)\r\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:145)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m indexer \u001b[39m=\u001b[39m [StringIndexer(inputCol\u001b[39m=\u001b[39mcolumn, outputCol\u001b[39m=\u001b[39mcolumn\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_index\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m column \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mbusiness_id\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[0;32m      2\u001b[0m pipeline \u001b[39m=\u001b[39m Pipeline(stages\u001b[39m=\u001b[39mindexer)\n\u001b[1;32m----> 3\u001b[0m transformed \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mfit(reviews_df)\u001b[39m.\u001b[39mtransform(reviews_df)\n\u001b[0;32m      4\u001b[0m transformed\u001b[39m.\u001b[39mselect([\u001b[39m'\u001b[39m\u001b[39mbusiness_id\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39muser_id\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mbusiness_id_index\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39muser_id_index\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mstars\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mcategories\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[0;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pyspark\\ml\\pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    132\u001b[0m     dataset \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39mtransform(dataset)\n\u001b[0;32m    133\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# must be an Estimator\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     model \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39;49mfit(dataset)\n\u001b[0;32m    135\u001b[0m     transformers\u001b[39m.\u001b[39mappend(model)\n\u001b[0;32m    136\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[0;32m    206\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pyspark\\ml\\wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset: DataFrame) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m JM:\n\u001b[1;32m--> 383\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[0;32m    384\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    385\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pyspark\\ml\\wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_java_obj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 380\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o736.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 111.0 failed 1 times, most recent failure: Lost task 0.0 in stage 111.0 (TID 454) (192.168.0.134 executor driver): TaskResultLost (result lost from block manager)\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:424)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:345)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:373)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:345)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3120)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r\n\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3120)\r\n\tat org.apache.spark.ml.feature.StringIndexer.countByValue(StringIndexer.scala:204)\r\n\tat org.apache.spark.ml.feature.StringIndexer.sortByFreq(StringIndexer.scala:212)\r\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:242)\r\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:145)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n"
     ]
    }
   ],
   "source": [
    "indexer = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in ['business_id', 'user_id']]\n",
    "pipeline = Pipeline(stages=indexer)\n",
    "transformed = pipeline.fit(reviews_df).transform(reviews_df)\n",
    "transformed.select(['business_id', 'user_id','business_id_index', 'user_id_index','stars','categories'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spliting the dataset into training and testing subsets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting a seed value as 3 to make randomsplit output deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training, test) = transformed.randomSplit([0.8, 0.2],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Average: 3.8922836512555232\n"
     ]
    }
   ],
   "source": [
    "# Compute the global mean\n",
    "average = training.select('user_id','stars').withColumn(\"user_id\",lit(1)).groupBy('user_id').mean()\n",
    "global_average = average.select('avg(stars)').collect()[0][0]\n",
    "\n",
    "print(\"Global Average:\",str(global_average))\n",
    "global_mean = training.agg(avg('stars')).collect()[0][0]\n",
    "print(\"Global Average:\",str(global_mean))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction using global average for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.2260849010325352\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the root mean squared error if we use the global average as a prediction for all rating, Our model should perform better than this\n",
    "\n",
    "\n",
    "test_avg = test.withColumn('prediction',lit(global_mean))\n",
    "    \n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"stars\",predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(test_avg)\n",
    "\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic ALS model\n",
    "The Apache Spark library provides various parameters for the ALS (Alternating Least Squares) algorithm, including:\n",
    "\n",
    "- rank: the number of latent factors used in the model (default value: 10).\n",
    "- maxIter: the maximum number of iterations to run (default value: 10).\n",
    "- regParam: the regularization parameter used in ALS (default value: 1.0).\n",
    "- implicitPrefs: a boolean value that indicates whether to use the explicit feedback ALS variant or the one adapted for implicit feedback data (default value: false, which means using explicit feedback).\n",
    "- alpha: a parameter that applies to the implicit feedback variant of ALS, determining the baseline confidence in preference observations (default value: 1.0).\n",
    "- nonnegative: a boolean value that specifies whether to use nonnegative constraints for least squares (default value: false)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ALS_262e2c7ef4d1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#ALS model with default values\n",
    "als=ALS(userCol=\"user_id_index\",itemCol=\"business_id_index\",ratingCol=\"stars\",coldStartStrategy=\"drop\",nonnegative=True)\n",
    "als.setSeed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the default model\n",
    "model=als.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the stars for test set \n",
    "predictions=model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE=1.3534108396501865\n"
     ]
    }
   ],
   "source": [
    "#evaludate default model\n",
    "evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"stars\",predictionCol=\"prediction\")\n",
    "rmse=evaluator.evaluate(predictions)\n",
    "print(\"RMSE=\"+str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE with latent factor 5 is=1.3524617852548415\n",
      "RMSE with latent factor 30 is=1.3238220621391332\n",
      "RMSE with latent factor 50 is=1.3182668564134439\n",
      "RMSE with latent factor 100 is=1.3092697928682182\n"
     ]
    }
   ],
   "source": [
    "ranks=[5,30,50,100]\n",
    "for rank in ranks:\n",
    "    als = ALS(rank=rank,userCol=\"user_id_index\",itemCol=\"business_id_index\",ratingCol=\"stars\",coldStartStrategy=\"drop\",nonnegative=True)\n",
    "    als.setSeed(2)\n",
    "    model = als.fit(training)\n",
    "    evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"stars\",predictionCol=\"prediction\")\n",
    "    predictions=model.transform(test)\n",
    "    rmse=evaluator.evaluate(predictions)\n",
    "    print(\"RMSE with latent factor \"+str(rank) +\" is=\"+str(rmse))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE with latent factor 100 and maxIter 10 is=1.309269792868218\n",
      "RMSE with latent factor 100 and maxIter 15 is=1.2994357989444387\n",
      "RMSE with latent factor 100 and maxIter 20 is=1.2990339815403404\n"
     ]
    }
   ],
   "source": [
    "epcohs=[10,15,20]\n",
    "for epcoh in epcohs:\n",
    "    als = ALS(rank=100,maxIter=epcoh,userCol=\"user_id_index\",itemCol=\"business_id_index\",ratingCol=\"stars\",coldStartStrategy=\"drop\",nonnegative=True)\n",
    "    als.setSeed(2)\n",
    "    model = als.fit(training)\n",
    "    evaluator=RegressionEvaluator(metricName=\"rmse\",labelCol=\"stars\",predictionCol=\"prediction\")\n",
    "    predictions=model.transform(test)\n",
    "    rmse=evaluator.evaluate(predictions)\n",
    "    print(\"RMSE with latent factor 100 and maxIter \"+str(epcoh) +\" is=\"+str(rmse))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALS Model With Bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+------------------+---------------------+\n",
      "|stars|         user_mean|         item_mean|user_item_interaction|\n",
      "+-----+------------------+------------------+---------------------+\n",
      "|  4.0| 3.652173913043478|3.6923076923076925|   0.5478020459043527|\n",
      "|  5.0|               4.0| 4.415384615384616|  0.47689903587090754|\n",
      "|  4.0|3.6666666666666665| 4.068493150684931|  0.15712383390392537|\n",
      "|  2.0|3.3684210526315788|               3.5|  -0.9761374013760555|\n",
      "|  3.0|               3.0| 3.933333333333333|  -0.0410496820778099|\n",
      "|  1.0|               1.0|               4.0| -0.10771634874447678|\n",
      "|  5.0|               4.0|               4.7|  0.19228365125552305|\n",
      "|  5.0| 3.652173913043478|3.5555555555555554|   1.6845541826564898|\n",
      "|  1.0|               1.0| 3.604651162790698|  0.28763248846482536|\n",
      "|  4.0|               4.0|   4.0817843866171| -0.18950073536157674|\n",
      "+-----+------------------+------------------+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute user mean and item mean on the training set\n",
    "user_mean = training.groupBy('user_id_index').agg(avg('stars').alias('user_mean'))\n",
    "item_mean = training.groupBy('business_id_index').agg(avg('stars').alias('item_mean'))\n",
    "#remove bias from training set\n",
    "interactions = training.join(user_mean, 'user_id_index').join(item_mean, 'business_id_index')\n",
    "interactions = interactions.withColumn('user_item_interaction', col('stars') - col('user_mean') - col('item_mean') + global_mean)\n",
    "interactions.select('stars','user_mean','item_mean','user_item_interaction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE with latent factor 5 is=1.18701500128988\n",
      "RMSE with latent factor 10 is=1.1856944430863605\n",
      "RMSE with latent factor 30 is=1.1831676942255596\n",
      "RMSE with latent factor 50 is=1.182823374677896\n",
      "RMSE with latent factor 100 is=1.1813151492298182\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='stars', predictionCol='user_item_interaction_recomputed')\n",
    "\n",
    "ranks=[5,10,30,50,100]\n",
    "for rank in ranks:\n",
    "    als = ALS(maxIter=20,rank=rank,userCol=\"user_id_index\",itemCol=\"business_id_index\",ratingCol=\"user_item_interaction\",coldStartStrategy=\"drop\",nonnegative=True)\n",
    "    als.setSeed(2)\n",
    "    model = als.fit(interactions)\n",
    "    predictions=model.transform(test).withColumnRenamed('prediction','predicted_rating')\n",
    "    predictions=  predictions.join(user_mean, 'user_id_index').join(item_mean, 'business_id_index')\n",
    "    predictions = predictions.withColumn('user_item_interaction_recomputed', col('predicted_rating') + col('user_mean') + col('item_mean') - global_mean)\n",
    "    rmse=evaluator.evaluate(predictions)\n",
    "    print(\"RMSE with latent factor \"+str(rank) +\" is=\"+str(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(maxIter=20,rank=100,userCol=\"user_id_index\",itemCol=\"business_id_index\",ratingCol=\"user_item_interaction\",coldStartStrategy=\"drop\",nonnegative=True)\n",
    "als.setSeed(2)\n",
    "model = als.fit(interactions)\n",
    "predictions=model.transform(test).withColumnRenamed('prediction','predicted_rating')\n",
    "predictions=  predictions.join(user_mean, 'user_id_index').join(item_mean, 'business_id_index')\n",
    "predictions = predictions.withColumn('user_item_interaction_recomputed', col('predicted_rating') + col('user_mean') + col('item_mean') - global_mean)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is=1.1813151492298182\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol='stars', predictionCol='user_item_interaction_recomputed')\n",
    "rmse=evaluator.evaluate(predictions)\n",
    "print(\"RMSE is=\"+str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation of predictions: 0.9202296973985253\n",
      "Standard deviation of stars: 1.140661507264479\n",
      "Mean of predictions: 3.9739221582961832\n",
      "Mean of stars: 3.8865761405816848\n",
      "Standard Deviation ratio  0.8067508998400492\n"
     ]
    }
   ],
   "source": [
    "std_dev = predictions.select(stddev('user_item_interaction_recomputed')).collect()[0][0]\n",
    "print('Standard deviation of predictions:', std_dev)\n",
    "\n",
    "std_dev_stars = predictions.select(stddev('stars')).collect()[0][0]\n",
    "print('Standard deviation of stars:', std_dev_stars)\n",
    "\n",
    "mean_prediction = predictions.select(mean('user_item_interaction_recomputed')).collect()[0][0]\n",
    "print('Mean of predictions:', mean_prediction)\n",
    "\n",
    "mean_stars = predictions.select(mean('stars')).collect()[0][0]\n",
    "print('Mean of stars:', mean_stars)\n",
    "\n",
    "print('Standard Deviation ratio ', std_dev/std_dev_stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Best Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations():\n",
    "    \"\"\"\n",
    "    Returns top recommendations for a user.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    :py:class:`pyspark.sql.DataFrame`\n",
    "    a DataFrame of (itemCol, recommendations), where recommendations are\n",
    "    stored as an array of ('name','business_id', 'stars', 'categories') Rows.\n",
    "    \"\"\"\n",
    "    test = model.recommendForAllUsers(10).filter(col('user_id_index')==30).select(\"recommendations\").take(10)\n",
    "    topRestaurants = []\n",
    "    for item in test[0][0]:        \n",
    "        topRestaurants.append(item.business_id_index)\n",
    "    \n",
    "    schema = StructType([StructField(\"business_id_index\",IntegerType(),True)])\n",
    "    restaurants = spark.createDataFrame(topRestaurants,IntegerType()).toDF(\"business_id_index\")\n",
    "    return restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_transformed_list():\n",
    "    transformed\\\n",
    "    .select(['name', 'user_id', 'stars', 'categories'])\\\n",
    "    .filter(col('user_id_index')==30)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_top10_recommendations(restaurants):\n",
    "    \"\"\"\n",
    "    Displays the top 10 restaurant recommendations.\n",
    "    \"\"\"\n",
    "    restaurants\\\n",
    "    .join(transformed, on = 'business_id_index', how = 'inner')\\\n",
    "    .select([ 'name','business_id', 'stars', 'categories'])\\\n",
    "    .drop_duplicates(subset=['name'])\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Top Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+\n",
      "|                name|         business_id|stars|          categories|\n",
      "+--------------------+--------------------+-----+--------------------+\n",
      "|       Casa de Mateo|09Hl38pDGEe1uSBNi...|  4.0|Restaurants, Mexican|\n",
      "|        Chez Di Vito|qwmlZkohv0MhBi9g9...|  4.0|Restaurants, Italian|\n",
      "|Da Franco Ristorante|F190EgYriOkncHd0v...|  5.0|Italian, Restaurants|\n",
      "|     L'Gros Luxe NDG|1aL-ic0cpqS3FkvSj...|  4.0|Comfort Food, Res...|\n",
      "|          L'Oeufrier|BMa7bY_ZdgDlVaM2Z...|  4.0|Breakfast & Brunc...|\n",
      "|             Mandy's|LtyoPfxpvcF_9e9wM...|  5.0|Restaurants, Vega...|\n",
      "|Pains Farcis Tianjin|len7Tn8eoi1KhUXbv...|  1.0|Specialty Food, R...|\n",
      "|Restaurant Xi'An ...|HVd718KlG6VdY5oh5...|  1.0|Restaurants, Chinese|\n",
      "|       Resto Chillax|ubLn_FrFygzcbhXTD...|  5.0|Restaurants, Cafe...|\n",
      "|               Tommy|JFnJyu4pdIGfXovYt...|  5.0|Breakfast & Brunc...|\n",
      "+--------------------+--------------------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "top10_recommendations = get_recommendations()\n",
    "display_top10_recommendations(top10_recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
